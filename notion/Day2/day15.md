# Model Comparison Arena (ëª¨ë¸ ë¹„êµ ë„êµ¬ ì œì‘)

# 0. ëª©í‘œ

<aside>
ğŸ’¡

**ë‹¤ì–‘í•œ LLM ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì‘ë‹µì„ ë¹„êµí•˜ëŠ” Streamlit ì•± êµ¬í˜„**

1. Snowflake Cortex AIë¥¼ í™œìš©í•œ ëª¨ë¸ ì‹¤í–‰
2. ë‘ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ì‹œê°„(Latency) ë° í† í° ìˆ˜ ë¹„êµ
3. Streamlitì„ í™œìš©í•œ ì§ê´€ì ì¸ ë¹„êµ UI êµ¬ì„±

</aside>

# 1. ê°œìš” ë° í•„ìš”ì„± (Overview)

- **Week 2(Chatbots)**ë¥¼ ë§ˆë¬´ë¦¬í•˜ë©°, ì‹¤ì œ ì–´í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ì§ˆë¬¸ì¸ **"ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?"**ì— ë‹µí•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.
- RAG(Week 3) êµ¬ì¶• ì „, ê° ëª¨ë¸ì˜ ì¥ë‹¨ì (ì†ë„ vs í’ˆì§ˆ, ë¹„ìš© vs ì„±ëŠ¥)ì„ ì§ì ‘ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# 2. Streamlit ì•± êµ¬í˜„ (Implementation)

## 2-1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì • ë° ëª¨ë¸ ì‹¤í–‰ í•¨ìˆ˜

- Snowflake Cortexì˜ `ai_complete` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì„ í˜¸ì¶œí•˜ê³ , ì‘ë‹µ ì‹œê°„ê³¼ í† í° ìˆ˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

    ```python
    import streamlit as st
    import time
    import json
    from snowflake.snowpark.functions import ai_complete

    # Connect to Snowflake
    try:
        from snowflake.snowpark.context import get_active_session
        session = get_active_session()
    except:
        from snowflake.snowpark import Session
        session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

    def run_model(model: str, prompt: str) -> dict:
        """ëª¨ë¸ ì‹¤í–‰ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        start = time.time()

        # Cortex Complete í•¨ìˆ˜ í˜¸ì¶œ
        df = session.range(1).select(
            ai_complete(model=model, prompt=prompt).alias("response")
        )

        # ê²°ê³¼ íŒŒì‹±
        rows = df.collect()
        response_raw = rows[0][0]
        response_json = json.loads(response_raw)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = response_json.get("choices", [{}])[0].get("messages", "") if isinstance(response_json, dict) else str(response_json)

        latency = time.time() - start
        tokens = int(len(text.split()) * 4/3)  # í† í° ìˆ˜ ì¶”ì • (1 word â‰ˆ 1.33 tokens)

        return {
            "latency": latency,
            "tokens": tokens,
            "response_text": text
        }
    ```

    - `ai_complete`: Snowflakeì—ì„œ ì œê³µí•˜ëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜
    - `time.time()`: API í˜¸ì¶œ ì „í›„ì˜ ì‹œê°„ì„ ì¸¡ì •í•˜ì—¬ Latency ê³„ì‚°
    - `len(text.split()) * 4/3`: ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° ê³µì‹ í™œìš©

## 2-2. ë¹„êµ UI êµ¬ì„± (Side-by-Side UI)

- ë‘ ê°œì˜ ëª¨ë¸ì„ ì„ íƒí•˜ê³  ê²°ê³¼ë¥¼ ë‚˜ë€íˆ ë³´ì—¬ì£¼ê¸° ìœ„í•´ `st.columns`ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

    ```python
    # ëª¨ë¸ ëª©ë¡ ì •ì˜
    llm_models = [
        "llama3-8b", "llama3-70b", "mistral-7b", "mixtral-8x7b",
        "claude-3-5-sonnet", "claude-haiku-4-5", "openai-gpt-5", "openai-gpt-5-mini"
    ]

    st.title(":material/compare: Select Models")
    col_a, col_b = st.columns(2)

    # Model A ì„ íƒ
    col_a.write("**Model A**")
    model_a = col_a.selectbox("Model A", llm_models, key="model_a", label_visibility="collapsed")

    # Model B ì„ íƒ (ê¸°ë³¸ê°’: ë‘ ë²ˆì§¸ ëª¨ë¸)
    col_b.write("**Model B**")
    model_b = col_b.selectbox("Model B", llm_models, key="model_b", index=1, label_visibility="collapsed")
    ```

    - `st.columns(2)`: í™”ë©´ì„ ì¢Œìš° 2ë¶„í• í•˜ì—¬ ë¹„êµì— ìµœì í™”ëœ ë ˆì´ì•„ì›ƒ êµ¬ì„±
    - `key`: ìœ„ì ¯ ê³ ìœ  ì‹ë³„ì (model_a, model_b)

## 2-3. ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ

- ê°™ì€ í”„ë¡¬í”„íŠ¸ë¡œ ë‘ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰(Sequential Execution)í•˜ê³  ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

    ```python
    # ì±„íŒ… ì…ë ¥
    st.divider()
    if prompt := st.chat_input("Enter your message to compare models"):
        # ìˆœì°¨ ì‹¤í–‰ (Model A -> Model B)
        with st.status(f"Running {model_a}..."):
            result_a = run_model(model_a, prompt)
        with st.status(f"Running {model_b}..."):
            result_b = run_model(model_b, prompt)

        # ê²°ê³¼ ì €ì¥ (Session State)
        st.session_state.latest_results = {"prompt": prompt, "model_a": result_a, "model_b": result_b}
        st.rerun()
    ```

    - `st.status`: ì‹¤í–‰ ì¤‘ì„ì„ ì‚¬ìš©ìì—ê²Œ ì‹œê°ì ìœ¼ë¡œ ì•Œë¦¼
    - `st.session_state`: ì‹¤í–‰ ì™„ë£Œ í›„ ë¦¬ëŸ°(rerun) ë˜ë”ë¼ë„ ê²°ê³¼ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©

# 3. í•µì‹¬ í¬ì¸íŠ¸ ë° ê³ ë ¤ì‚¬í•­

## í™˜ê²½ í˜¸í™˜ì„± (Cross-Environment Compatibility)
- `try-except` êµ¬ë¬¸ì„ í†µí•´ ë¡œì»¬ ê°œë°œ í™˜ê²½ê³¼ Snowflake ë‚´ë¶€(SiS) í™˜ê²½ ëª¨ë‘ì—ì„œ ì½”ë“œ ìˆ˜ì • ì—†ì´ ë™ì‘í•©ë‹ˆë‹¤.

## ì„±ëŠ¥ ì¸¡ì • (Metrics)
- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„±ì´ ì•„ë‹ˆë¼, **ì‘ë‹µ ì†ë„(Latency)**ì™€ **ì¶œë ¥ëŸ‰(Tokens)**ì„ í•¨ê»˜ ë³´ì—¬ì¤Œìœ¼ë¡œì¨ ë¹„ìš©/ì„±ëŠ¥ íš¨ìœ¨ì„±ì„ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì§€í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

# ì‹¤í–‰ ê²°ê³¼

## ì‹¤í–‰ ì½”ë“œ

Streamlit ì‹¤í–‰ ì½”ë“œ = python -m streamlit run íŒŒì¼ëª….py

ì˜ˆì‹œ : `python -m streamlit run app/day15.py`
